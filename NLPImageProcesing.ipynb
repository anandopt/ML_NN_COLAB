{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPImageProcesing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandopt/ML_NN_COLAB/blob/master/NLPImageProcesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZcDUI3BAU7qS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "a82f03fa-7491-4bdf-ff37-bad059d3a7fb"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f2NwMR65V5Kf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Python Code for Impage Processing for chapter 03.\n",
        "\n",
        "The NN is trained on CIFAR10 dataset available in keras library.\n",
        "\n",
        "matplotlib.pyplot library is import for visualization of the data."
      ]
    },
    {
      "metadata": {
        "id": "fhfl9wBhWHY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8619e443-a249-49b4-ab8f-133ebc571a53"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "juvZisJkXERe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Cifar10 data has 10 classes"
      ]
    },
    {
      "metadata": {
        "id": "CSvggO3kW__Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# List of names for each CIFAR10 class\n",
        "cifar10_class_names = {\n",
        "    0: \"Plane\",\n",
        "    1: \"Car\",\n",
        "    2: \"Bird\",\n",
        "    3: \"Cat\",\n",
        "    4: \"Deer\",\n",
        "    5: \"Dog\",\n",
        "    6: \"Frog\",\n",
        "    7: \"Horse\",\n",
        "    8: \"Boat\",\n",
        "    9: \"Truck\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJA6vJUSXMSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the entire data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Loop through each picture in the data set\n",
        "for i in range(1000):\n",
        "    # Grab an image from the data set\n",
        "    sample_image = x_train[i]\n",
        "    # Grab the image's expected class id\n",
        "    image_class_number = y_train[i][0]\n",
        "    # Look up the class name from the class id\n",
        "    image_class_name = cifar10_class_names[image_class_number]\n",
        "\n",
        "#     # Draw the image as a plot\n",
        "#     plt.imshow(sample_image)\n",
        "#     # Label the image\n",
        "#     plt.title(image_class_name)\n",
        "#     # Show the plot on the screen\n",
        "#     plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwD9R4Y3fe-a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the dataset and normalize the data between 0 and 1"
      ]
    },
    {
      "metadata": {
        "id": "cQLIQ7le6GfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Import libraries required to model the NN\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4W_nfKFt6PDV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "# Our labels are single values from 0 to 9.\n",
        "# Instead, we want each label to be an array with on element set to 1 and and the rest set to 0.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DA51Es43UpJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create model and add Layers"
      ]
    },
    {
      "metadata": {
        "id": "KJ7NoZbGU1fS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "#####Compile the model \n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "##### Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mr5c0l_ubkx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save the neuraal network structure and save the NN's trained weights."
      ]
    },
    {
      "metadata": {
        "id": "erTNpisEboMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_structure = model.to_json()\n",
        "f = Path(\"model_structure.json\")\n",
        "f.write_text(model_structure)\n",
        "\n",
        "# Save neural network's trained weights\n",
        "model.save_weights(\"model_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e288SUoxcpQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the json file that contains the model's structure\n",
        "f = Path(\"model_structure.json\")\n",
        "model_structure = f.read_text()\n",
        "\n",
        "# Recreate the Keras model object from the json data\n",
        "model = model_from_json(model_structure)\n",
        "\n",
        "# Re-load the model's trained weights\n",
        "model.load_weights(\"model_weights.h5\")\n",
        "\n",
        "# Load an image file to test, resizing it to 32x32 pixels (as required by this model)\n",
        "img = image.load_img(\"frog.png\", target_size=(32, 32))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "image_to_test = image.img_to_array(img)\n",
        "\n",
        "# Add a fourth dimension to the image (since Keras expects a list of images, not a single image)\n",
        "list_of_images = np.expand_dims(image_to_test, axis=0)\n",
        "\n",
        "# Make a prediction using the model\n",
        "results = model.predict(list_of_images)\n",
        "\n",
        "# Since we are only testing one image, we only need to check the first result\n",
        "single_result = results[0]\n",
        "\n",
        "# We will get a likelihood score for all 10 possible classes. Find out which class had the highest score.\n",
        "most_likely_class_index = int(np.argmax(single_result))\n",
        "class_likelihood = single_result[most_likely_class_index]\n",
        "\n",
        "# Get the name of the most likely class\n",
        "class_label = class_labels[most_likely_class_index]\n",
        "\n",
        "# Print the result\n",
        "print(\"This is image is a {} - Likelihood: {:2f}\".format(class_label, class_likelihood))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8vX-M7tfDVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        "\n",
        "# Load Keras' VGG16 model that was pre-trained against the ImageNet database\n",
        "model = vgg16.VGG16()\n",
        "\n",
        "# Load the image file, resizing it to 224x224 pixels (required by this model)\n",
        "img = image.load_img(\"bay.jpg\", target_size=(224, 224))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Add a fourth dimension (since Keras expects a list of images)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Normalize the input image's pixel values to the range used when training the neural network\n",
        "x = vgg16.preprocess_input(x)\n",
        "\n",
        "# Run the image through the deep neural network to make a prediction\n",
        "predictions = model.predict(x)\n",
        "\n",
        "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
        "predicted_classes = vgg16.decode_predictions(predictions)\n",
        "\n",
        "print(\"Top predictions for this image:\")\n",
        "\n",
        "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
        "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjQvBzKRgvzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import joblib\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        "\n",
        "# Path to folders with training data\n",
        "dog_path = Path(\"training_data\") / \"dogs\"\n",
        "not_dog_path = Path(\"training_data\") / \"not_dogs\"\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load all the not-dog images\n",
        "for img in not_dog_path.glob(\"*.png\"):\n",
        "    # Load the image from disk\n",
        "    img = image.load_img(img)\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    image_array = image.img_to_array(img)\n",
        "\n",
        "    # Add the image to the list of images\n",
        "    images.append(image_array)\n",
        "\n",
        "    # For each 'not dog' image, the expected value should be 0\n",
        "    labels.append(0)\n",
        "\n",
        "# Load all the dog images\n",
        "for img in dog_path.glob(\"*.png\"):\n",
        "    # Load the image from disk\n",
        "    img = image.load_img(img)\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    image_array = image.img_to_array(img)\n",
        "\n",
        "    # Add the image to the list of images\n",
        "    images.append(image_array)\n",
        "\n",
        "    # For each 'dog' image, the expected value should be 1\n",
        "    labels.append(1)\n",
        "\n",
        "# Create a single numpy array with all the images we loaded\n",
        "x_train = np.array(images)\n",
        "\n",
        "# Also convert the labels to a numpy array\n",
        "y_train = np.array(labels)\n",
        "\n",
        "# Normalize image data to 0-to-1 range\n",
        "x_train = vgg16.preprocess_input(x_train)\n",
        "\n",
        "# Load a pre-trained neural network to use as a feature extractor\n",
        "pretrained_nn = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Extract features for each image (all in one pass)\n",
        "features_x = pretrained_nn.predict(x_train)\n",
        "\n",
        "# Save the array of extracted features to a file\n",
        "joblib.dump(features_x, \"x_train.dat\")\n",
        "\n",
        "# Save the matching array of expected values to a file\n",
        "joblib.dump(y_train, \"y_train.dat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Dl13FLEg-Qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "# Load data set\n",
        "x_train = joblib.load(\"x_train.dat\")\n",
        "y_train = joblib.load(\"y_train.dat\")\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Save neural network structure\n",
        "model_structure = model.to_json()\n",
        "f = Path(\"model_structure.json\")\n",
        "f.write_text(model_structure)\n",
        "\n",
        "# Save neural network's trained weights\n",
        "model.save_weights(\"model_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6Upg5OPhBTy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "from pathlib import Path\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.applications import vgg16\n",
        "\n",
        "# Load the json file that contains the model's structure\n",
        "f = Path(\"model_structure.json\")\n",
        "model_structure = f.read_text()\n",
        "\n",
        "# Recreate the Keras model object from the json data\n",
        "model = model_from_json(model_structure)\n",
        "\n",
        "# Re-load the model's trained weights\n",
        "model.load_weights(\"model_weights.h5\")\n",
        "\n",
        "# Load an image file to test, resizing it to 64x64 pixels (as required by this model)\n",
        "img = image.load_img(\"not_dog.png\", target_size=(64, 64))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "image_array = image.img_to_array(img)\n",
        "\n",
        "# Add a forth dimension to the image (since Keras expects a bunch of images, not a single image)\n",
        "images = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "# Normalize the data\n",
        "images = vgg16.preprocess_input(images)\n",
        "\n",
        "# Use the pre-trained neural network to extract features from our test image (the same way we did to train the model)\n",
        "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "features = feature_extraction_model.predict(images)\n",
        "\n",
        "# Given the extracted features, make a final prediction using our own model\n",
        "results = model.predict(features)\n",
        "\n",
        "# Since we are only testing one image with possible class, we only need to check the first result's first element\n",
        "single_result = results[0][0]\n",
        "\n",
        "# Print the result\n",
        "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAJpmtxKhFoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from base64 import b64encode\n",
        "\n",
        "import googleapiclient.discovery\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Settings\n",
        "IMAGE_FILE = \"road_sign.jpg\"\n",
        "CREDENTIALS_FILE = \"credentials.json\"\n",
        "\n",
        "# Connect to the Google Cloud-ML Service\n",
        "credentials = GoogleCredentials.from_stream(CREDENTIALS_FILE)\n",
        "service = googleapiclient.discovery.build('vision', 'v1', credentials=credentials)\n",
        "\n",
        "# Read file and convert it to a base64 encoding\n",
        "with open(IMAGE_FILE, \"rb\") as f:\n",
        "    image_data = f.read()\n",
        "    encoded_image_data = b64encode(image_data).decode('UTF-8')\n",
        "\n",
        "# Create the request object for the Google Vision API\n",
        "batch_request = [{\n",
        "    'image': {\n",
        "        'content': encoded_image_data\n",
        "    },\n",
        "    'features': [\n",
        "        {\n",
        "            'type': 'LABEL_DETECTION'\n",
        "        }\n",
        "    ]\n",
        "}]\n",
        "request = service.images().annotate(body={'requests': batch_request})\n",
        "\n",
        "# Send the request to Google\n",
        "response = request.execute()\n",
        "\n",
        "# Check for errors\n",
        "if 'error' in response:\n",
        "    raise RuntimeError(response['error'])\n",
        "\n",
        "# Print the results\n",
        "labels = response['responses'][0]['labelAnnotations']\n",
        "\n",
        "for label in labels:\n",
        "    print(label['description'], label['score'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eOylWLahNhT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from base64 import b64encode\n",
        "\n",
        "import googleapiclient.discovery\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Change this values to match your project\n",
        "IMAGE_FILE = \"text.png\"\n",
        "CREDENTIALS_FILE = \"credentials.json\"\n",
        "\n",
        "# Connect to the Google Cloud-ML Service\n",
        "credentials = GoogleCredentials.from_stream(CREDENTIALS_FILE)\n",
        "service = googleapiclient.discovery.build('vision', 'v1', credentials=credentials)\n",
        "\n",
        "# Read file and convert it to a base64 encoding\n",
        "with open(IMAGE_FILE, \"rb\") as f:\n",
        "    image_data = f.read()\n",
        "    encoded_image_data = b64encode(image_data).decode('UTF-8')\n",
        "\n",
        "# Create the request object for the Google Vision API\n",
        "batch_request = [{\n",
        "    'image': {\n",
        "        'content': encoded_image_data\n",
        "    },\n",
        "    'features': [\n",
        "        {\n",
        "            'type': 'TEXT_DETECTION'\n",
        "        }\n",
        "    ]\n",
        "}]\n",
        "request = service.images().annotate(body={'requests': batch_request})\n",
        "\n",
        "# Send the request to Google\n",
        "response = request.execute()\n",
        "\n",
        "# Check for errors\n",
        "if 'error' in response:\n",
        "    raise RuntimeError(response['error'])\n",
        "\n",
        "# Print the results\n",
        "extracted_texts = response['responses'][0]['textAnnotations']\n",
        "\n",
        "# Print the first piece of text found in the image\n",
        "extracted_text = extracted_texts[0]\n",
        "print(extracted_text['description'])\n",
        "\n",
        "# Print the location where the text was detected\n",
        "print(extracted_text['boundingPoly'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}