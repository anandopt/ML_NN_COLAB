{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandopt/ML_NN_COLAB/blob/master/ObjectImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Ju3g09gZCm7I",
        "colab_type": "code",
        "outputId": "a558fe3a-6786-4321-f952-f380b36bd34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 213.4MB 129kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 29.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HDbN9sA5UbHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MACE2ZcclpA",
        "colab_type": "code",
        "outputId": "ef12e5bb-7565-4a9b-d6f7-e5efe1e311f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "### Mount Gdrive to google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ceHyt13udfS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Data processing **\n",
        "convert raw data into torch tensor and normalize\n",
        "\n",
        "*_tasks* will be called to convert downloaded raw data into normalized tensor data."
      ]
    },
    {
      "metadata": {
        "id": "FPYzl_LVffiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "_tasks = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))        \n",
        "])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RWAH1pxnkY-l",
        "colab_type": "code",
        "outputId": "eb97d35b-be8e-46c6-b0af-756f00167076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "##### Download data and tranform it using _tasks\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "cifar = CIFAR10(\"data\", download=True, train=True, transform=_tasks)\n",
        "print(cifar)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Split: train\n",
            "    Root Location: data\n",
            "    Transforms (if any): Compose(\n",
            "                             ToTensor()\n",
            "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
            "                         )\n",
            "    Target Transforms (if any): None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fWmG5v2EtHiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting the dimensions of components of tensor\n",
        "In order to check the dimensions of each component of tensor, make it iterable,\n",
        "\n",
        "call \"next\" to get one component of iterable,\n",
        "\n",
        "convert the outcome into numpy array using ***np.squeeze(next(iterable).numpy())***"
      ]
    },
    {
      "metadata": {
        "id": "AQkFJxnsliPe",
        "colab_type": "code",
        "outputId": "3a0746c3-b793-4506-dccf-f13c5a97a2c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cifariter = iter(cifar)\n",
        "# print(next(cifariter))\n",
        "a,_ = next(cifariter)\n",
        "a=np.squeeze(a.numpy())\n",
        "a.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "xZTDETc2uN1k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the tensor data into training and validation sets\n",
        "\n",
        "80% data is used for the training and rest will be used for the validation purposes"
      ]
    },
    {
      "metadata": {
        "id": "Ri6nOsfvpql9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split = int(0.8*len(cifar))\n",
        "index_list = list(range(len(cifar)))\n",
        "train_idx, validate_idx = index_list[:split], index_list[split:]\n",
        "train_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6AES3l5jvdDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now create training and validation data using *SubsetRandomSampler*\n"
      ]
    },
    {
      "metadata": {
        "id": "d0pd7yRKvQ6E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "validate_sampler = SubsetRandomSampler(validate_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M31YLWyjwSoS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **DataLoader iterators**\n",
        "A utility of PyTorch is DataLoader iterators provides the ability to batch, \n",
        "shuffle and load the data in parallel using multiprocessing workers. \n",
        "\n",
        "DataLoader is defined in **\n",
        "\n",
        "For the purpose of evaluating our model, the data will be partitioned training and validation sets."
      ]
    },
    {
      "metadata": {
        "id": "KyWRHppSxAQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(cifar, batch_size=256, sampler=train_sampler)\n",
        "validate_loader = DataLoader(cifar, batch_size=256, sampler=validate_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJP4Xsqh5mwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Architecture of CNN**\n",
        "We will create the architecture with three convolutional layers for low-level feature extraction, \n",
        "\n",
        "three pooling layers for maximum information extraction, \n",
        "\n",
        "and two linear layers for linear classification."
      ]
    },
    {
      "metadata": {
        "id": "KAFMCvyi6YET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural network architecture in PyTorch is defined in a class which inherits the base class from nn package called Module (nn.Module). \n",
        "\n",
        "nn.Module class allows us to implement, access, and call a number of methods easily. \n",
        "\n",
        "All the layers are generally defined inside the constructor of the class, and the forward propagation steps inside the forward function.\n",
        "\n",
        "Inside the forward function, we will use the sigmoid activation function in the hidden layer (which can be accessed from the nn module)."
      ]
    },
    {
      "metadata": {
        "id": "eiAFZfLNBDag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    \n",
        "    ### Define the layers\n",
        "    self.conv1 = nn.Conv2d(3,16,3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16,32,3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32,64,3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.linear1 = nn.Linear(1024, 512)\n",
        "    self.linear2 = nn.Linear(512, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    ##### Reshape the x\n",
        "    x = x.view(-1, 1024)\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return(x)\n",
        "\n",
        "model = Model()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SAMYKYb1vr5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now define the loss function and optimization algorithm to be used\n",
        "\n",
        "Cross Entropy Loss is used as the loss function\n",
        "\n",
        "Stochastic Descent method is used for the optimaztion\n"
      ]
    },
    {
      "metadata": {
        "id": "jRDuwTBNv8mH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUH2YMWFOnV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following training data is used to train the model.\n",
        "\n",
        "The trained model is evaluated on the Evaluation data\n"
      ]
    },
    {
      "metadata": {
        "id": "TOq8HlhhPfFs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1,31):\n",
        "  train_loss, valid_loss = [], [] ### Array to store training and validation loss for each epoch\n",
        "  ### Training starts here\n",
        "  model.train()\n",
        "  for data, target in train_loader:\n",
        "    optimizer.zero_grad()     ### Start all gradients with zero\n",
        "    output = model(data)\n",
        "    loss = loss_func(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss.item())\n",
        "    \n",
        "  ### Evaluating the trainedd model\n",
        "  for data, target in validate_loader:\n",
        "    output = model(data)\n",
        "    loss = loss_func(output, target)\n",
        "    valid_loss.append(loss.item())\n",
        "    \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQ_JkwRqwvZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Make predictions using validation data**"
      ]
    },
    {
      "metadata": {
        "id": "4G4W5G5_uscZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0889c6a0-6b00-42da-99e9-cb478f600513"
      },
      "cell_type": "code",
      "source": [
        "dataiter = iter(validate_loader)\n",
        "data, lebel = dataiter.next()\n",
        "print(lebel)\n",
        "output = model(data)\n",
        "# a = torch.max(output,1)\n",
        "# a\n",
        "_, preds_tensor = torch.max(output,1)\n",
        "preds = np.squeeze(preds_tensor.numpy())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([9, 0, 5, 5, 2, 1, 0, 0, 7, 1, 5, 1, 1, 5, 1, 3, 8, 7, 2, 6, 5, 6, 3, 1,\n",
            "        5, 9, 7, 0, 0, 1, 7, 1, 3, 2, 8, 8, 4, 8, 4, 1, 7, 2, 4, 1, 4, 4, 9, 3,\n",
            "        2, 9, 0, 3, 5, 0, 3, 7, 2, 0, 0, 7, 7, 8, 7, 0, 5, 9, 0, 2, 7, 6, 6, 6,\n",
            "        3, 5, 5, 3, 2, 2, 6, 1, 3, 1, 4, 1, 0, 2, 4, 6, 4, 6, 2, 7, 2, 4, 1, 5,\n",
            "        9, 9, 2, 7, 3, 5, 8, 6, 1, 9, 7, 3, 3, 0, 8, 8, 0, 0, 1, 6, 7, 3, 2, 3,\n",
            "        0, 9, 7, 7, 6, 4, 1, 3, 5, 7, 0, 5, 2, 4, 8, 8, 4, 6, 5, 4, 1, 6, 2, 3,\n",
            "        2, 6, 7, 0, 4, 8, 3, 0, 7, 5, 5, 5, 6, 4, 8, 0, 9, 7, 1, 3, 7, 1, 7, 0,\n",
            "        0, 3, 4, 1, 4, 9, 3, 6, 8, 6, 6, 5, 1, 1, 7, 3, 4, 6, 0, 5, 4, 9, 5, 0,\n",
            "        2, 5, 9, 4, 3, 4, 4, 8, 0, 9, 5, 2, 5, 0, 2, 8, 2, 9, 5, 1, 3, 4, 2, 9,\n",
            "        5, 0, 8, 2, 0, 4, 8, 7, 3, 2, 4, 0, 1, 4, 8, 6, 9, 0, 3, 8, 0, 6, 8, 0,\n",
            "        4, 2, 8, 9, 5, 2, 1, 4, 4, 3, 9, 4, 9, 4, 9, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ydce0dsRz9hy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5248e7c7-333d-4efd-9c51-007f3a65e3f9"
      },
      "cell_type": "code",
      "source": [
        "print(\"Actual: \", lebel[:10])\n",
        "print(\"Predicted: \", preds[:10])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual:  tensor([9, 0, 5, 5, 2, 1, 0, 0, 7, 1])\n",
            "Predicted:  [9 0 5 5 3 1 0 0 7 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2NGvQkQV0Hri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}